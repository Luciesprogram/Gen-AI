{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaMxkemS/HBsR1r8vyjYND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luciesprogram/Gen-AI/blob/main/Extract_Images%2C_Text%2C_Tables_from_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realtime MultiModal Usecases | Exract Images, Tables, Text from Documents | Multimodal summarizer"
      ],
      "metadata": {
        "id": "gIxhNjbQBkKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QEUy9hHx-TQ7",
        "outputId": "c9c9f105-a2f4-4e98-b58c-6bf68b8148c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting unstructured[all-docs]\n",
            "  Downloading unstructured-0.18.26-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.4.4)\n",
            "Collecting filetype (from unstructured[all-docs])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured[all-docs])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.13.5)\n",
            "Collecting emoji (from unstructured[all-docs])\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured[all-docs])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured[all-docs])\n",
            "  Downloading python_iso639-2025.11.16-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langdetect (from unstructured[all-docs])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured[all-docs])\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured[all-docs])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.15.0)\n",
            "Collecting unstructured-client (from unstructured[all-docs])\n",
            "  Downloading unstructured_client-0.42.6-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured[all-docs])\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.1)\n",
            "Collecting pi_heif (from unstructured[all-docs])\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.10)\n",
            "Collecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting msoffcrypto-tool (from unstructured[all-docs])\n",
            "  Downloading msoffcrypto_tool-5.4.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.16.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.2.2)\n",
            "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-10.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting unstructured-inference>=1.1.1 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-1.1.4-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting effdet (from unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting onnxruntime>=1.19.0 (from unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs])\n",
            "  Downloading google_cloud_vision-3.11.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pypdf (from unstructured[all-docs])\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.6.1)\n",
            "Collecting onnx>=1.17.0 (from unstructured[all-docs])\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer_six-20260107-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (1.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (0.36.0)\n",
            "Requirement already satisfied: opencv-python>=4.12 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (4.12.0.88)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (2.9.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (1.0.22)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[all-docs]) (1.16.3)\n",
            "Collecting pypdfium2 (from unstructured-inference>=1.1.1->unstructured[all-docs])\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.8)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (0.24.0+cu126)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.43.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.26.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
            "Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool->unstructured[all-docs]) (43.0.3)\n",
            "Collecting olefile>=0.46 (from msoffcrypto-tool->unstructured[all-docs])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (2025.11.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.3)\n",
            "Collecting Deprecated (from pikepdf->unstructured[all-docs])\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (2025.11.12)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (0.28.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[all-docs]) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (4.12.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->unstructured-inference>=1.1.1->unstructured[all-docs]) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (75.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->unstructured-inference>=1.1.1->unstructured[all-docs]) (0.22.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->unstructured-inference>=1.1.1->unstructured[all-docs]) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs])\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (2.23)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->unstructured-inference>=1.1.1->unstructured[all-docs]) (3.0.3)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-1.1.4-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_cloud_vision-3.11.0-py3-none-any.whl (529 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.1/529.1 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msoffcrypto_tool-5.4.2-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pdfminer_six-20260107-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m151.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-10.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.16.2-py3-none-any.whl (19 kB)\n",
            "Downloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.11.16-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.18.26-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.42.6-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=92e309a8d6757e00a4583501fe81e2c43d219726d64d674ef4dcac0cbdc7b314\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, pi_heif, pdf2image, olefile, mypy-extensions, marshmallow, langdetect, humanfriendly, emoji, Deprecated, backoff, typing-inspect, python-pptx, python-oxmsg, pikepdf, onnx, coloredlogs, unstructured-client, pdfminer.six, onnxruntime, msoffcrypto-tool, dataclasses-json, unstructured, google-cloud-vision, unstructured-inference, effdet\n",
            "Successfully installed Deprecated-1.3.1 XlsxWriter-3.2.9 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 effdet-0.4.1 emoji-2.15.0 filetype-1.2.0 google-cloud-vision-3.11.0 humanfriendly-10.0 langdetect-1.0.9 marshmallow-3.26.2 msoffcrypto-tool-5.4.2 mypy-extensions-1.1.0 olefile-0.47 onnx-1.20.0 onnxruntime-1.23.2 pdf2image-1.17.0 pdfminer.six-20260107 pi_heif-1.1.1 pikepdf-10.1.0 pypandoc-1.16.2 pypdf-6.5.0 pypdfium2-5.3.0 python-docx-1.2.0 python-iso639-2025.11.16 python-magic-0.4.27 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.14.3 typing-inspect-0.9.0 unstructured-0.18.26 unstructured-client-0.42.6 unstructured-inference-1.1.4 unstructured.pytesseract-0.3.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4dfc41b1cb5c42a4bf0578ff7088f7a8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlIgrv1SCUOh",
        "outputId": "97c574fe-daf4-407e-bdeb-24aa309f54dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,233 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,586 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,864 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Fetched 38.4 MB in 5s (7,150 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGjJ7yWhCp9z",
        "outputId": "2ef2f79b-3e63-4a86-b0a7-eb5ba2011859"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 78 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Fetched 186 kB in 1s (179 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sj65iFDF28G",
        "outputId": "8a404846-6644-45b3-9e4a-30aab8baceb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "tesseract-ocr-eng set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libimagequant0 libraqm0 python3-olefile\n",
            "Suggested packages:\n",
            "  python-pil-doc\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libimagequant0 libleptonica-dev libraqm0 libtesseract-dev\n",
            "  python3-olefile python3-pil tesseract-ocr-script-latn\n",
            "0 upgraded, 8 newly installed, 0 to remove and 78 not upgraded.\n",
            "Need to get 35.1 MB of archives.\n",
            "After this operation, 107 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraqm0 amd64 0.7.0-4ubuntu1 [11.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-olefile all 0.46-3 [33.8 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pil amd64 9.0.1-1ubuntu0.3 [419 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Fetched 35.1 MB in 3s (13.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121719 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "Preparing to unpack .../1-libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../2-libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libraqm0:amd64.\n",
            "Preparing to unpack .../3-libraqm0_0.7.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../4-libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../5-python3-olefile_0.46-3_all.deb ...\n",
            "Unpacking python3-olefile (0.46-3) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../6-python3-pil_9.0.1-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../7-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up python3-olefile (0.46-3) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured-pytesseract\n",
        "!pip install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKZ0A8AWF8Eh",
        "outputId": "3b4c497c-c823-44c5-f26f-5e913963474a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.15)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from unstructured-pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-pytesseract) (11.3.0)\n",
            "Collecting tesseract-ocr\n",
            "  Downloading tesseract-ocr-0.0.1.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from tesseract-ocr) (3.0.12)\n",
            "Building wheels for collected packages: tesseract-ocr\n",
            "  Building wheel for tesseract-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract-ocr: filename=tesseract_ocr-0.0.1-cp312-cp312-linux_x86_64.whl size=188046 sha256=c14cf5ced8f9569cd2a368ac023b78f6e39d02f7be3ca0c4c933b7f85a38e964\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/96/8d/1e2b502793a5aa288a72ffdc5e96e9ebd22a72be6c0af195b5\n",
            "Successfully built tesseract-ocr\n",
            "Installing collected packages: tesseract-ocr\n",
            "Successfully installed tesseract-ocr-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf"
      ],
      "metadata": {
        "id": "h43nG1IKGA7Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements=partition_pdf(\n",
        "    filename=\"/content/data/cj.pdf\",\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udGY_HsPGEtJ",
        "outputId": "5978097f-3c8f-41c5-b3f4-23dda2f25920"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JODX5Nh6HUXL",
        "outputId": "6a9ce068-8bed-461f-95f9-07743586205c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Text at 0x7d413c937b30>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c9377a0>,\n",
              " <unstructured.documents.elements.Header at 0x7d413c7dec60>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c8576e0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c8562a0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c856d50>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c856bd0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c857710>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c855010>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c857890>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c934f20>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c934050>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c935ee0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c7dc260>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c937860>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413c7dd220>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c7dda90>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c7dd7c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c7dfad0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c857680>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8557f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8545c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c856ed0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c855f70>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c857860>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c856960>,\n",
              " <unstructured.documents.elements.Footer at 0x7d413c854d10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c7de4e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9372f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c857830>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c854350>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c855760>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8561e0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c8ce1e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cfe60>,\n",
              " <unstructured.documents.elements.Footer at 0x7d413c8cd2b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c935a30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c855550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c936ab0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c857320>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cc7a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cf9e0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c8cdee0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8ce270>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c9e3740>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cc050>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9e0680>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c854ad0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413c8571d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c7dc3e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c856b40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9e01a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9e2750>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9e30e0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c9e35c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8ccb00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cda30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8cfb60>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c8cea20>,\n",
              " <unstructured.documents.elements.Text at 0x7d413ca1f800>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8ccf80>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413ca1e9c0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413ca1d160>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c9e2db0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413c8ce0c0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413c937020>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8ceb70>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9e3800>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1c770>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1e840>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1d700>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1f770>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1e9f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413ca1e960>,\n",
              " <unstructured.documents.elements.Image at 0x7d413ca1d8e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1ff50>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413ca1d6d0>,\n",
              " <unstructured.documents.elements.Footer at 0x7d413ca1d1c0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c8cce60>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413ca1e060>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1f6e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1d2e0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413ca1eb40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1f1a0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413ca1e120>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8925d0>,\n",
              " <unstructured.documents.elements.Footer at 0x7d413c893b60>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c8560c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1c470>,\n",
              " <unstructured.documents.elements.Title at 0x7d413ca1e780>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1c380>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1cad0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c892480>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1f680>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca1ddf0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c893bc0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c96b8c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96b020>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c96a7b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96a210>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c969d30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96a720>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c96aff0>,\n",
              " <unstructured.documents.elements.Table at 0x7d413ca1cda0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413ca1fc80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9369f0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c8921e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c893f50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96bfe0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c968e90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96ac60>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96b500>,\n",
              " <unstructured.documents.elements.Table at 0x7d413ca1ec30>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d413c857560>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c969100>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c8935c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96bb90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96ae40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96a120>,\n",
              " <unstructured.documents.elements.Table at 0x7d413c968f20>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96b1a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96a630>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c9685f0>,\n",
              " <unstructured.documents.elements.Footer at 0x7d413c9684a0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c96a3c0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c935b20>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c968320>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c968710>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c9688c0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c96b770>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa30710>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa30f50>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa325d0>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa33680>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fc35e80>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c96ad20>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c969400>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c96a1e0>,\n",
              " <unstructured.documents.elements.Text at 0x7d412a357770>,\n",
              " <unstructured.documents.elements.Image at 0x7d413ca59e80>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d415fa30fb0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca59910>,\n",
              " <unstructured.documents.elements.Title at 0x7d413ca5bbc0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413ca5a330>,\n",
              " <unstructured.documents.elements.Title at 0x7d415fa31d90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa33ec0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa32ff0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa330b0>,\n",
              " <unstructured.documents.elements.Footer at 0x7d415fa33c20>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c9d5bb0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c968350>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413ca5bc50>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d415fa31790>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d415fa33320>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354a70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3552e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3561e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357dd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3571a0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354d10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357a40>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357d40>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354e00>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a355190>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a355d30>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a355f10>,\n",
              " <unstructured.documents.elements.Footer at 0x7d412a3556d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c96bb60>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413ca5a660>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d415fa33050>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d415fa33380>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c8cd730>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357d70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357cb0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3562d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3572c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a356990>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354590>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a356540>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a357e60>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d415fa33e00>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c9e3fe0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c96a990>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d413c969cd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a356a80>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3579b0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a355280>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a3557f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354c20>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a356f60>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a354200>,\n",
              " <unstructured.documents.elements.ListItem at 0x7d412a356e70>,\n",
              " <unstructured.documents.elements.Header at 0x7d412a355fa0>,\n",
              " <unstructured.documents.elements.Title at 0x7d413c7df020>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a356390>,\n",
              " <unstructured.documents.elements.Title at 0x7d412a357e30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a354c50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a356600>,\n",
              " <unstructured.documents.elements.Title at 0x7d412a355370>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a354410>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a357ef0>,\n",
              " <unstructured.documents.elements.Header at 0x7d412a3577d0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c96ac00>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d412a3560f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a357800>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d412a3570b0>,\n",
              " <unstructured.documents.elements.Header at 0x7d412a3579e0>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a356ae0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7d412a357ec0>,\n",
              " <unstructured.documents.elements.Header at 0x7d412a356150>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a3575f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a356510>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a3549e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a357f80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a354bf0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d412a355820>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa17b30>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a354740>,\n",
              " <unstructured.documents.elements.Table at 0x7d413c8916a0>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa176e0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa15df0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa17890>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa15730>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa14b00>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a356840>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16030>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa15820>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa15250>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa163f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16450>,\n",
              " <unstructured.documents.elements.Table at 0x7d415fa14d70>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa155b0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16cc0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa147a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa14ef0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa15f10>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa17200>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa14980>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa15160>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa14c20>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa143b0>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa171a0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa173b0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16ba0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16f90>,\n",
              " <unstructured.documents.elements.Table at 0x7d415fa17da0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa148f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa14bf0>,\n",
              " <unstructured.documents.elements.Title at 0x7d415fa15a00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa167b0>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa17500>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa16ab0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa162d0>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa16960>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa14470>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa16420>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa17d40>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa175f0>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa17c50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa161b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa166c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d415fa17260>,\n",
              " <unstructured.documents.elements.Header at 0x7d415fa16510>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa154f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa17d10>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa15370>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa17950>,\n",
              " <unstructured.documents.elements.Text at 0x7d415fa17ad0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c869ee0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c868c80>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c869be0>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c86a780>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c868260>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c869760>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c86b320>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c868e90>,\n",
              " <unstructured.documents.elements.Text at 0x7d413c868b90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c86b5f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d412a3578f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa14e00>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c86a090>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c869670>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c86b7d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c869100>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c86af60>,\n",
              " <unstructured.documents.elements.Header at 0x7d413c86a9f0>,\n",
              " <unstructured.documents.elements.Image at 0x7d415fa17440>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c869eb0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c86a5d0>,\n",
              " <unstructured.documents.elements.Image at 0x7d413c86b800>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c86b500>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c86a990>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7d413c869d30>,\n",
              " <unstructured.documents.elements.Header at 0x7d413c86b440>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Header=[]\n",
        "Footer=[]\n",
        "Title=[]\n",
        "NarrativeText=[]\n",
        "Text=[]\n",
        "ListItem=[]\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))\n"
      ],
      "metadata": {
        "id": "2duB5egiN_5F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5E1p8WjOXCJ",
        "outputId": "50d9a5c7-39cb-4617-e9d5-04da3c0b4e45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Minesh Mathew1 Dimosthenis Karatzas2 C.V. Jawahar1 1CVIT, IIIT Hyderabad, India 2Computer Vision Center, UAB, Spain minesh.mathew@research.iiit.ac.in, dimos@cvc.uab.es, jawahar@iiit.ac.in',\n",
              " 'We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions deﬁned on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is pre- sented. We report several baseline results by adopting exist- ing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve speciﬁcally on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org',\n",
              " 'Q: What date is seen on the seal at the top of the letter? A: 23 sep 1970',\n",
              " 'Q: Which company address is mentioned on the letter? A: Great western sugar Co.',\n",
              " 'Research in Document Analysis and Recognition (DAR) is generally focused on information extraction tasks that aim to convert information in document images into ma- chine readable form, such as character recognition [10], ta- ble extraction [22] or key-value pair extraction [30]. Such algorithms tend to be designed as task speciﬁc blocks, blind to the end-purpose the extracted information will be used for.',\n",
              " 'Progressing independently in such information extrac- tion processes has been quite successful, although it is not necessarily true that holistic document image understanding can be achieved through a simple constructionist approach, building upon such modules. The scale and complexity of the task introduce difﬁculties that require a different point of view.',\n",
              " 'In this article we introduce Document Visual Question Answering (DocVQA), as a high-level task dynamically driving DAR algorithms to conditionally interpret docu- ment images. By doing so, we seek to inspire a “purpose- driven” point of view in DAR research. In case of Docu- ment VQA, as illustrated in Figure 1, an intelligent reading system is expected to respond to ad-hoc requests for infor- mation, expressed in natural language questions by human',\n",
              " 'Figure 1: Example question-answer pairs from DocVQA. Answering questions in the new dataset require models not just to read text but interpret it within the layout/structure of the document.',\n",
              " 'users. To do so, reading systems should not only extract and interpret the textual (handwritten, typewritten or printed) content of the document images, but exploit numerous other visual cues including layout (page structure, forms, tables), non-textual elements (marks, tick boxes, separators, dia- grams) and style (font, colours, highlighting), to mention just a few.',\n",
              " 'Departing from generic VQA [13] and Scene Text VQA [35, 5] approaches, the document images warrants a differ- ent approach to exploit all the above visual cues, making use of prior knowledge of the implicit written communi- cation conventions used, and dealing with the high-density semantic information conveyed in such images. Answers in case of document VQA cannot be sourced from a closed dictionary, but they are inherently open ended.',\n",
              " 'Previous approaches on bringing VQA to the documents',\n",
              " 'domain have either focused on speciﬁc document elements such as data visualisations [19, 21] or on speciﬁc collections such as book covers [28]. In contrast to such approaches, we recast the problem to its generic form, and put forward a large scale, varied collection of real documents.',\n",
              " 'Main contributions of this work can be summarized as following:',\n",
              " 'Machine reading comprehension (MRC) and open- domain question answering (QA) are two problems which are being actively pursued by Natural Language Process- ing (NLP) and Information Retrieval (IR) communities. In MRC the task is to answer a natural language ques- tion given a question and a paragraph (or a single docu- ment) as the context. In case of open domain QA, no spe- ciﬁc context is given and answer need to be found from a large collection (say Wikipedia) or from Web. MRC is of- ten modelled as an extractive QA problem where answer is deﬁned as a span of the context on which the ques- tion is deﬁned. Examples of datsets for extractive QA in- clude SQuAD 1.1 [32], NewsQA [37] and Natural Ques- tions [27]. MS MARCO [29] is an example of a QA dataset for abstractive QA where answers need to be generated not extracted. Recently Transformer based pretraining meth- ods like Bidirectional Encoder Representations from Trans- formers (BERT) [9] and XLNet [41] have helped to build QA models outperforming Humans on reading comprehen- sion on SQuAD [32]. In contrast to QA in NLP where con- text is given as computer readable strings, contexts in case',\n",
              " 'Visual Question Answering (VQA) aims to provide an accurate natural language answer given an image and a nat- ural language question. VQA has attracted an intense re- search effort over the past few years [13, 1, 17]. Out of a large body of work on VQA, scene text VQA branch is the most related to our work. Scene text VQA refers to VQA systems aiming to deal with cases where understand- ing scene text instances is necessary to respond to the ques- tions posed. The ST-VQA [5] and TextVQA [35] datasets were introduced in parallel in 2019 and were quickly fol-',\n",
              " 'lowed by more research [36, 11, 39].',\n",
              " 'The ST-VQA dataset [5] has 31,000+ questions over 23,000+ images collected from different public data sets. The TextVQA dataset [35] has 45,000+ questions over 28,000+ images sampled from speciﬁc categories of the OpenImages dataset [25] that are expected to contain text. Another dataset named OCR-VQA [28] comprises more than 1 million question-answer pairs over 207K+ images of book covers. The questions in this dataset are domain spe- ciﬁc, generated based on template questions and answers extracted from available metadata.',\n",
              " 'Scene text VQA methods [16, 11, 35, 12] typically make use of pointer mechanisms in order to deal with out-of- vocabulary (OOV) words appearing in the image and pro- vide the open answer space required. This goes hand in hand with the use of word embeddings capable of encoding OOV words into a pre-deﬁned semantic space, such as Fast- Text [6] or BERT [9]. More recent, top-performing methods in this space include M4C [16] and MM-GNN [11] models.',\n",
              " 'Parallelly there have been works on certain domain spe- ciﬁc VQA tasks which require to read and understand text in the images. The DVQA dataset presented by Kaﬂe et al. [20, 19] comprises synthetically generated images of bar charts and template questions deﬁned automatically based on the bar chart metadata. The dataset contains more than three million question-answer pairs over 300,000 images.',\n",
              " 'FigureQA [21] comprises over one million yes or no questions, grounded on over 100,000 images. Three differ- ent types of charts are used: bar, pie and line charts. Similar to DVQA, images are synthetically generated and questions are generated from templates. Another related QA task is Textbook Question Answering (TQA) [23] where multiple choice questions are asked on multimodal context, includ- ing text, diagrams and images. Here textual information is provided in computer readable format.',\n",
              " 'Compared to these existing datasets either concerning VQA on real word images, or domain speciﬁc VQA for charts or book covers, the proposed DocVQA comprise document images. The dataset covers a multitude of differ- ent document types that include elements like tables, forms and ﬁgures , as well as a range of different textual, graphical and structural elements.',\n",
              " 'In this section we explain data collection and annotation process and present statistics and analysis of DocVQA.',\n",
              " 'Document Images: Images in the dataset are sourced from documents in UCSF Industry Documents Library1. The documents are organized under different industries and',\n",
              " '1https://www.industrydocuments.ucsf.edu/',\n",
              " 'Figure 2: Document images we use in the dataset come from 6071 documents spanning many decades, of a variety of types, originating from 5 different industries. We use documents from UCSF Industry Documents Library.',\n",
              " 'further under different collections. We downloaded doc- uments from different collections and hand picked pages from these documents for use in the dataset. Majority of documents in the library are binarized and the binarization has taken on a toll on the image quality. We tried to min- imize binarized images in DocVQA since we did not want poor image quality to be a bottleneck for VQA. We also prioritized pages with tables, forms, lists and ﬁgures over pages which only have running text.',\n",
              " 'The ﬁnal set of images in the dataset are drawn from pages of 6,071 industry documents. We made use of doc- uments from as early as 1900 to as recent as 2018. ( Fig- ure 2b). Most of the documents are from the 1960-2000 pe- riod and they include typewritten, printed, handwritten and born-digital text. There are documents from all 5 major in- dustries for which the library hosts documents — tobacco, food, drug, fossil fuel and chemical. We use many docu- ments from food and nutrition related collections, as they have a good number of non-binarized images. . See Fig- ure 2a for industry wise distribution of the 6071 documents used. The documents comprise a wide variety of document types as shown in Figure 2c.',\n",
              " 'answers for the questions. In this stage workers were also required to assign one or more question types to each ques- tion. The different question types in DocVQA are discussed in subsection 3.2. During the second stage, if the worker ﬁnds a question inapt owing to language issues or ambi- guity, an option to ﬂag the question was provided. Such questions are not included in the dataset.',\n",
              " 'If none of the answers entered in the ﬁrst stage match exactly with any of the answers from the second stage, the particular question is sent for review in a third stage. Here questions and answers are editable and the reviewer either accepts the question-answer (after editing if necessary) or ignores it. The third stage review is done by the authors themselves.',\n",
              " 'The DocVQA comprises 50,000 questions framed on 12,767 images. The data is split randomly in an 80−10−10 ratio to train, validation and test splits. The train split has 39,463 questions and 10,194 images, the validation split has 5,349 questions and 1,286 images and the test split has 5,188 questions and 1,287 images.',\n",
              " 'Questions and Answers: Questions and answers on the selected document images are collected with the help of re- mote workers, using a Web based annotation tool. The an- notation process was organized in three stages. In stage 1, workers were shown a document image and asked to de- ﬁne at most 10 question-answer pairs on it. We encouraged the workers to add more than one ground truth answer per question in cases where it is warranted. Workers were in- structed to ask questions which can be answered using text present in the image and to enter the answer verbatim from the document. This makes VQA on the DocVQA dataset an extractive QA problem similar to extractive QA tasks in NLP [32, 37] and VQA in case of ST-VQA [5].',\n",
              " 'As mentioned before, questions are tagged with ques- tion type(s) during the second stage of the annotation pro-',\n",
              " 'The second annotation stage aims to verify the data col- lected in the ﬁrst stage. Here a worker was shown an im- age and questions deﬁned on it in the ﬁrst stage (but not the answers from the ﬁrst stage), and was required to enter',\n",
              " 'cess. Figure 3 shows the 9 question types and percentage of questions under each type. A question type signiﬁes the type of data where the question is grounded. For example, ‘table/list’ is assigned if answering the question requires un- derstanding of a table or a list. If the information is in the form of a key:value, the ‘form’ type is assigned. ‘Layout’ is assigned for questions which require spatial/layout infor- mation to ﬁnd the answer. For example, questions asking for a title or heading, require one to understand structure of the document. If answer for a question is based on infor- mation in the form of sentences/paragraphs type assigned is ‘running text’. For all questions where answer is based on handwritten text, ‘handwritten’ type is assigned. Note that a question can have more than one type associated with it. (Examples from DocVQA for each question type are given in the supplementary.)',\n",
              " 'highest among the compared datasets. . In DocVQA 35,362 (70.72%) questions are unique. Figure 4a shows the top 15 most frequent questions and their frequencies. There are questions repeatedly being asked about dates, ti- tles and page numbers. A sunburst of ﬁrst 4 words of the questions is shown in Figure 6.',\n",
              " 'It can be seen that a large majority of questions start with “what is the”, asking for date, title, total, amount or name.',\n",
              " 'Distribution of answer lengths is shown in Figure 4e. We observe in the ﬁgure that both DocVQA and SQuAD 1.1 have a higher number of longer answers compared to the VQA datasets. The average answer length is 2.17.',\n",
              " '63.2% of the answers are unique , which is second only to SQuAD 1.1 (72.5%). The top 15 answers in the dataset are shown in Figure 4b.',\n",
              " 'In the following analysis we compare statistics of ques- tions, answers and OCR tokens with other similar datasets for VQA — VQA 2.0 [13], TextVQA [35] and ST-VQA [5] and SQuAD 1.1 [32] dataset for reading comprehension. Statistics for other datasets are computed based on their publicly available data splits. For statistics on OCR to- kens, for DocVQA we use OCR tokens generated by a commercial OCR solution. For VQA 2.0, TextVQA and ST-VQA we use OCR tokens made available by authors of LoRRA [35] and M4C [16] as part of the MMF [34] frame- work.',\n",
              " 'We observe that almost all of the top answers are nu- meric values, which is expected since there are a good num- ber of document images of reports and invoices. In Fig-',\n",
              " 'Figure 4d shows distribution of question lengths for questions in DocVQA compared to other similar datasets. The average question length is is 8.12, which is second',\n",
              " 'ure 4c we show the top 15 non numeric answers. These include named entities such as names of people, institutions and places. The word cloud on the left in Figure 5 shows frequent words in answers. Most common words are names of people and names of calendar months.',\n",
              " 'In Figure 4f we show the number of images (or ‘context’s in case of SQuAD 1.1) containing a particular number of text tokens. Average number of text tokens in an image or context is the highest in the case of DocVQA (182.75). It is considerably higher compared to SQuAD 1.1 where contexts are usually small paragraphs whose average length is 117.23. In case of VQA datasets which comprise real world images average number of OCR tokens is not more than 13. Word cloud on the right in Figure 5 shows the most common words spotted by the OCR on the images in DocVQA. We observe that there is high overlap between common OCR tokens and words in answers.',\n",
              " 'In this section we explain the baselines we use, including heuristics and trained models.',\n",
              " 'Heuristics we evaluate are: (i) Random answer: mea- sures performance when we pick a random answer from the answers in the train split. (ii) Random OCR token: perfor- mance when a random OCR token from the given document image is picked as the answer. (iii) Longest OCR token is',\n",
              " 'the case when the longest OCR token in the given document is selected as the answer. (iv) Majority answer measures the performance when the most frequent answer in the train split is considered as the answer.',\n",
              " 'We also compute following upper bounds: (i) Vocab UB: This upper bound measures performance upper bound one can get by predicting correct answers for the questions, provided the correct answer is present in a vocabulary of an- swers, comprising all answers which occur more than once in the train split. (ii) OCR substring UB: is the upper bound on predicting the correct answer provided the answer can be found as a substring in the sequence of OCR tokens. The sequence is made by serializing the OCR tokens recog- nized in the documents as a sequence separated by space, in top-left to bottom-right order. (iii) OCR subsequence UB: upper bound on predicting the correct answer, provided the answer is a subsequence of the OCR tokens’ sequence.',\n",
              " 'For evaluating performance of existing VQA models on DocVQA we employ two models which take the text present in the images into consideration while answering questions – Look, Read, Reason & Answer (LoRRA) [35] and Multimodal Multi-Copy Mesh(M4C) [16].',\n",
              " 'LoRRA: follows a bottom-up and top-down attention [3] scheme with additional bottom-up attention over OCR to- kens from the images. In LoRRA, tokens in a question are ﬁrst embedded using a pre-trained embedding (GloVe [31]) and then these tokens are iteratively encoded using an LSTM [15] encoder. The model uses two types of spa- tial features to represent the visual information from the images - (i) grid convolutional features from a Resnet- 152 [14] which is pre-trained on ImageNet [8] and (ii) fea- tures extracted from bounding box proposals from a Faster R-CNN [33] object detection model, pre-trained on Visual Genome [26]. OCR tokens from the image are embedded using a pre-trained word embedding (FastText [7]). An at- tention mechanism is used to compute an attention weighed average of the image features as well the OCR tokens’ em- beddings. These averaged features are combined and fed into an output module. The classiﬁcation layer of the model, predicts an answer either from a ﬁxed vocabulary (made from answers in train set) or copy an answer from a dynamic vocabulary which essentially is the list of OCR tokens in an image. Here the copy mechanism can copy only one of the OCR tokens from the image. Consequently it cannot out- put an answer which is a combination of two or more OCR tokens.',\n",
              " 'M4C: uses a multimodal transformer and an iterative an- swer prediction module. Here tokens in questions are em- bedded using a BERT model [9]. Images are represented using (i) appearance features of the objects detected using a Faster-RCNN pretrained on Visual Genome [26] and (ii)',\n",
              " 'location information - bounding box coordinates of the de- tected objects. Each OCR token recognized from the im- age is represented using (i) a pretrained word embedding (FastText), (ii) appearance feature of the token’s bounding box from the same Faster R-CNN which is used for ap- pearance features of objects (iii) PHOC [2] representation of the token and (iv) bounding box coordinates of the to- ken. Then these feature representations of the three entities (question tokens, objects and OCR tokens) are projected to a common, learned embedding space. Then a stack of Transformer [38] layers are applied over these features in the common embedding space. The multi-head self atten- tion in transformers enable both inter-entity and intra-entity attention. Finally, answers are predicted through iterative decoding in an auto-regressive manner. Here the ﬁxed vo- cabulary used for the closed answer space is made up of the most common answer words in the train split. Note that in this case the ﬁxed vocabulary comprises of answer words, not answers itself as in the case of LoRRA. At each step in the decoding, the decoded word is either an OCR token from the image or a word from the ﬁxed vocabulary of com- mon answer words.',\n",
              " 'In our experiments we use original LoRRA and M4C models and few variants of these models. Document images in DocVQA usually contain higher number of text tokens compared to images in scene text VQA datasets. Hence we try out larger dynamic vocabularies (i.e. more OCR tokens are considered from the images) for both LoRRA and M4C. For both the models we also evaluate performance when no ﬁxed vocabulary is used.',\n",
              " 'Since the notion of visual objects in real word images is not directly applicable in case of document images, we also try out variants of LoRRA and M4C where features of objects are omitted.',\n",
              " 'In addition to the VQA models which can read text, we try out extractive question answering / reading comprehen- sion models from NLP. In particular, we use BERT [9] ques- tion answering models. BERT is a method of pre-training language representations from unlabelled text using trans- formers [38]. These pretrained models can then be used for downstream tasks with just an additional output layer. In the case of extractive Question Answering, this is an output layer to predict start and end indices of the answer span.',\n",
              " 'In this section we explain evaluation metrics and our ex- perimental settings and report results of experiments.',\n",
              " 'Two evaluation metrics we use are Average Normal- ized Levenshtein Similarity (ANLS) and Accuracy (Acc.).',\n",
              " 'ANLS was originally proposed for evaluation of VQA on ST-VQA [4]. The Accuracy metric measures percentage of questions for which the predicted answer matches exactly with any of the target answers for the question. Accuracy metric awards a zero score even when the prediction is only a little different from the target answer. Since no OCR is perfect, we propose to use ANLS as our primary evaluation metric, so that minor answer mismatches stemming from OCR errors are not severely penalized.',\n",
              " 'For measuring human performance , we collect answers for all questions in test split, with help a few volunteers from our institution.',\n",
              " 'In all our experiments including heuristics and trained baselines, OCR tokens we use are extracted using a com- mercial OCR application. For the heuristics and upper bounds we use a vocabulary 4,341 answers which occur more than once in the train split.',\n",
              " 'For LoRRA and M4C models we use ofﬁcial implemen- tations available as part of the MMF framework [34]. The training settings and hyper parameters are same as the ones reported in the original works. The ﬁxed vocabulary we use for LoRRA is same as the vocabulary we use for computing vocabulary based heuristics and upper bounds. For M4C the ﬁxed vocabulary we use is a vocabulary of the 5,000 most frequent words from the answers in the train split.',\n",
              " 'For QA using BERT, three pre-trained BERT mod- els2 from the Transformers library [40] are used. The models we use are bert-base-uncased, bert-large-uncased- whole-word-masking and bert-large-uncased-whole-word- masking-ﬁnetuned-squad. We abbreviate the model names as bert-base, bert-large and bert-large-squad respectively. Among these, bert-large-squad is a pre-trained model which is also ﬁnetuned on SQuAD 1.1 for question answering. In',\n",
              " '2https://huggingface.co/transformers/ pretrained_models.html',\n",
              " 'case of extractive question answering or reading compre- hension datasets ‘contexts’ on which questions are asked are passages of electronic text. But in DocVQA ‘contexts’ are document images. Hence to ﬁnetune the BERT QA models on DocVQA we need to prepare the data in SQuAD style format where the answer to a question is a ‘span’ of the context, deﬁned by start and end indices of the answer. To this end we ﬁrst serialize the OCR tokens recognized on the document images to a single string, separated by space, in top-left to bottom-right order. To approximate the answer spans we follow an approach proposed in TriviaQA [18], which is to ﬁnd the ﬁrst match of the answer string in the serialized OCR string.',\n",
              " 'Results of all heuristic approaches and upper bounds are reported in Table 1. We can see that none of the heuristics get even a 1% accuracy on the validation or test splits.',\n",
              " 'OCR substring UB yields more than 85% accuracy on both validation and test splits. It has a downside that the substring match in all cases need not be an actual answer match. For example if the answer is “2” which is the most common answer in the dataset, it will match with a “2” in “2020” or a “2” in “2pac”. This is the reason why we evalu- ate the OCR subsequence UB. An answer is a sub sequence of the serialized OCR output for around 76% of the ques- tions in both validation and test splits.',\n",
              " 'The bert-base model is ﬁnetuned on DocVQA on 2 Nvidia GeForce 1080 Ti GPUs, for 2 epochs, with a batch size of 32. We use Adam optimizer [24] with a learning rate of 5e − 05. The bert-large and bert-large-squad models are ﬁnetuned on 4 GPUs for 6 epochs with a batch size of 8, and a learning rate of 2e − 05.',\n",
              " 'Table 3: Performance of BERT question answering models. A BERTLARGE model which is ﬁne tuned on both SQuAD 1.1 [32] and DocVQA performs the best.',\n",
              " 'Results of our trained VQA baselines are shown in Ta- ble 2. First rows for both the methods report results of the original model proposed by the respective authors. In case of LoRRA the original setting proposed by the authors yields the best results compared to the variants we try out. With no ﬁxed vocabulary, the performance of the model drops sharply suggesting that the model primarily relies on the ﬁxed vocabulary to output answers. Larger dynamic vo- cabulary results in a slight performance drop suggesting that incorporating more OCR tokens from the document images does little help. Unlike LoRRA, M4C beneﬁts from a larger dynamic vocabulary. Increasing the size of the dynamic vocabulary from 50 to 500 improves the ANLS by around 50%. And in case of M4C, the setting where features of ob- jects are omitted, performs slightly better compared to the original setting.',\n",
              " 'Results of the BERT question answering models are re- ported in Table 3. We observe that all BERT models per- form better than the best VQA baseline using M4C (last row in 2). The best performing model out of all the base- lines analysed is the bert-large-squad model, ﬁnetuned on DocVQA. Answers predicted by this model match one of',\n",
              " 'Figure 7: Qualitative results from our experiments. The leftmost example is a ‘layout’ type question answered correctly by the M4C model but erred by the BERT model. In the second example the BERT model correctly answers a question on a form while the M4C model fails. In case of the rightmost example, both models fail to understand a step by step illustration.',\n",
              " 'on the test split. We observe that the human performance is uniform while the models’ performance vary for different question types. In Figure 7 we show a few qualitative results from our experiments.',\n",
              " 'We introduce a new data set and an associated VQA task with the aim to inspire a “purpose-driven” approach in doc- ument image analysis and recognition research. Our base- lines and initial results motivate simultaneous use of visual and textual cues for answering questions asked on document images. This could drive methods that use the low-level cues (text, layout, arrangements) and high-level goals (pur- pose, relationship, domain knowledge) in solving problems of practical importance.',\n",
              " 'the target answers exactly for around 55% of the questions.',\n",
              " 'In Figure 8 we show performance by question type. We compare the best models among VQA models and BERT question answering models against the human performance',\n",
              " 'We thank Amazon for supporting the annotation effort, and Dr. R. Manmatha for many useful discussions and in- puts. This work is partly supported by MeitY, Government of India, the project TIN2017-89779-P, an Amazon AWS Research Award and the CERCA Programme.',\n",
              " 'As mentioned in Section 3.1 in the main paper, anno- tation process involves three stages. In Figure A.1, Fig- ure A.2 and Figure A.3 we show screen grabs from stage 1, stage 2 and stage 3 of the annotation process respectively.',\n",
              " 'We deﬁne 9 question types, based on the kind of rea- soning required to answer a question. Question types are assigned at the second stage of the annotation. We discuss the question types in Section 3.2. in the main paper.',\n",
              " 'Examples for types form, yes/no and layout are shown in Figure B.1. Examples for a question based on a handwrit- ten date in a form (types form and handwritten) are shown in Figure B.2. An example for a question based on informa- tion in the form of sentences or paragraphs ( type running text) is shown in Figure B.3. Examples for types photo- graph and table are shown in Figure B.4. An example for a question based on a plot (type ﬁgure) is shown in Fig- ure B.5. In all examples a crop of the original image is shown below the original image, for better viewing of the image region where the question is based on.',\n",
              " 'Here we show more qualitative results from our baseline experiments. These results supplement the Results section (Section 5.3 ) in the main paper.',\n",
              " 'Remember that BERT [9] question answering model is designed to answer questions asked on sentences or para- graphs of text ( reading comprehension). In Figure C.1 we show two examples where the model answers questions outside the ambit of reading comprehension style question answering. In Figure C.2 we show examples where the M4C [16] model outperforms the BERT model to answer questions based on text seen on pictures or photographs. Such questions are similar to questions in TextVQA [35] or ST-VQA [5] datasets where M4C model yield state-of- the-art results. In Figure C.3 we show an example where both the models yield inconsistent results when posed with questions of similar nature, highlighting lack of reasoning behind answering. In Figure C.4 we show two examples where both the M4C and BERT model fail to answer ques- tions which require understanding of a ﬁgure or a diagram. In Figure C.5 we show how OCR errors have resulted in wrong answers although the models manage to ground the questions correctly.',\n",
              " 'Q: Is it an existing item ? Question types: form and yes/no A: yes',\n",
              " 'Q: What is the date given at the top left? Question types: layout A: 03/17/98',\n",
              " 'Figure B.1: On the left is a question based on an yes/no check box. On the right, the question seeks for a date given at a particular spatial location — top left of the page.',\n",
              " 'Q: What is the date written next to RSM approval? Question types: form and handwritten A: 3-17-98',\n",
              " 'Figure B.2: Date is handwritten and it is shown in a key:value format.',\n",
              " 'Q: If the request needs to be warehoused by RJR, what needs to be done ? Question types: running text A: write to RJR',\n",
              " 'Figure B.3: Question is grounded on a sentence.',\n",
              " 'Q: Whose picture is given? Question types: photograph and layout A: Dr. Dwayne G. Westfall',\n",
              " 'Q: What is the average sucrose % for N level 501+ ? Question types: table A: 15.9',\n",
              " 'Figure B.4: On the left is a question asking for name of the person in the photograph. To answer the question on the right, one needs to parse the table and pick the value in the appropriate cell',\n",
              " 'Q: What is the highest value for “Intake, mg/1000kcal” plotted on the ‘X’ axis of the graph? Question types: ﬁgure A: 300',\n",
              " 'Figure B.5: Question is based on the plot shown at the bottom of the given image, asking for the highest value on the X axis',\n",
              " 'Q: What is the total cost for Fat cell size (Mt. SInai) in the -05 year ?',\n",
              " 'M4C best: 4400 BERT best: $35 , 864 Human: $35,864',\n",
              " 'GT: hawaiian fruit cake M4C best: island desserts (continued from cake BERT best: hawaiian fruit cake Human: hawaiian fruit cake',\n",
              " 'Figure C.1: Examples where BERT QA model [9] answers questions other than ‘running text’ type. On the left is a question based on a table and for the other question one needs to know the ‘ﬁrst recipe’ out of the two recipes shown. For the ﬁrst question the model gets the answer correct except for an extra space, and in case of the second one the predicted answer matches exactly with the ground truth answer.',\n",
              " 'GT: let yourself grow! M4C best: yourself grow! BERT best: < no prediction > Human: let yourself grow!',\n",
              " 'Q: What Tobacco brand of GPI is shown in the picture? GT: Prince M4C best: prince BERT best: < no prediction > Human: prince',\n",
              " 'Figure C.2: How does the M4C [16] model perform on questions based on pictures or photographs. Here we show two examples where the best variant of the M4C model outperform the BERT best model in answering ‘layout’ type questions seeking to read what is written in a logo/pack. The BERT model doesnt make any predictions for the questions.',\n",
              " 'Figure C.3: Contrasting results for similar questions. Here both the questions are based on the table at the bottom of the image. Both questions ask for ‘committee strength’ for a particular meeting (ﬁrst or last). Both models get the answer right for the ﬁrst one. But for the question on the right, the models predict same answer as the ﬁrst one (“6”) while the ground 21 truth is “5”. This suggests that the models’ predictions are not backed by a proper reasoning/grounding in all cases.',\n",
              " 'Q: What is the position above ”vice chairman” ? GT: chairman M4C best: legal counsel BERT best: legal counsel Human: chairman',\n",
              " 'Q: What is the highest value shown on the vertical axis? GT: 99.99 M4C best: 50 BERT best: 32 Human: 99.99',\n",
              " 'Figure C.4: Understanding ﬁgures and diagrams. In case of the question on the left, one needs to understand an organiza- tional hierarchy diagram. For the second question, one needs to know what a ‘vertical axis’ is, and then ﬁnd the largest value. Both the models fail to answer the questions.',\n",
              " 'Q: What is the name of the passenger? GT: dr. william j. darby M4C best: larry BERT best: larry Human: dr. william j. darry',\n",
              " 'Q: What is the date present in the memo ? GT: 1/7/77 M4C best: 1 7 77 BERT best: 1 / 7 Human: 1/7/77',\n",
              " 'Figure C.5: Impact of OCR errors. Here the models are able to ground the questions correctly on the relevant information in the image, but failed to get the answers correct owing to the OCR errors. In case of the question on the left, even the answer entered by the human volunteer is not exactly matching with the ground truth. In case of the second question, OCR has split the date into multiple tokens due to over segmentation, resulting in incorrect answers by both the models.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX0H0AgxOZ8k",
        "outputId": "d6514746-e0a6-42f1-f005-71f6b84ac109"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['• We introduce DocVQA, a large scale dataset of 12,767 document images of varied types and content, over which we have deﬁned 50,000 questions and answers. The questions deﬁned are categorised based on their reasoning requirements, allowing us to analyze how DocVQA methods fare for different question types.',\n",
              " '• We deﬁne and evaluate various baseline methods over the DocVQA dataset, ranging from simple heuristic methods and human performance analysis that allow us to deﬁne upper performance bounds given different assumptions, to state of the art Scene Text VQA mod- els and NLP models.',\n",
              " '[1] Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. C-VQA: A compositional split of the vi- sual question answering (VQA) v1. 0 dataset. arXiv preprint arXiv:1704.08243, 2017.',\n",
              " '[2] J. Almaz´an, A. Gordo, A. Forn´es, and E. Valveny. Word spotting and recognition with embedded attributes. TPAMI, 2014.',\n",
              " '[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering, 2017.',\n",
              " '[4] Ali Furkan Biten, Rub`en Tito, Andr´es Maﬂa, Llu´ıs G´omez, Marc¸al Rusi˜nol, Minesh Mathew, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. ICDAR 2019 com- petition on scene text visual question answering. CoRR, abs/1907.00490, 2019.',\n",
              " '[5] Ali Furkan Biten, Ruben Tito, Andres Maﬂa, Lluis Gomez, Marcal Rusinol, Ernest Valveny, C.V. Jawahar, and Dimos- thenis Karatzas. Scene text visual question answering. In ICCV, 2019.',\n",
              " '[6] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword infor- mation. Transactions of the Association for Computational Linguistics, 5.',\n",
              " '[7] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword infor- mation. Transactions of the Association for Computational Linguistics, 5, 2017.',\n",
              " '[8] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei- Fei. Imagenet: A large-scale hierarchical image database. In CPVR, 2009.',\n",
              " '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In ACL, 2019.',\n",
              " '[10] David Doermann, Karl Tombre, et al. Handbook of document image processing and recognition. Springer, 2014.',\n",
              " '[11] Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen. Multi-modal graph neural network for joint reasoning on vision and scene text. In CVPR, 2020.',\n",
              " '[12] Llu´ıs G´omez, Ali Furkan Biten, Rub`en Tito, Andr´es Maﬂa, and Dimosthenis Karatzas. Multimodal grid features and cell pointers for scene text visual question answering. arXiv preprint arXiv:2006.00923, 2020.',\n",
              " '[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing, 2016.',\n",
              " '[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.',\n",
              " '[15] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 1997.',\n",
              " '[16] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Mar- cus Rohrbach. Iterative answer prediction with pointer- augmented multimodal transformers for textvqa. In CVPR, 2020.',\n",
              " '[17] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elemen- tary visual reasoning. In CVPR, pages 2901–2910, 2017.',\n",
              " '[18] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettle- moyer. TriviaQA: A large scale distantly supervised chal- lenge dataset for reading comprehension. In ACL, 2017.',\n",
              " '[19] Kushal Kaﬂe, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via ques- tion answering. In CVPR, 2018.',\n",
              " '[20] Kushal Kaﬂe, Robik Shrestha, Scott Cohen, Brian Price, and Christopher Kanan. Answering questions about data visual- izations using efﬁcient bimodal fusion. In WACV, 2020.',\n",
              " '[21] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkin- son, ´Akos K´ad´ar, Adam Trischler, and Yoshua Bengio. Fig- ureqa: An annotated ﬁgure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017.',\n",
              " '[22] Isaak Kavasidis, Carmelo Pino, Simone Palazzo, Francesco Rundo, Daniela Giordano, P Messina, and Concetto Spamp- inato. A saliency-based convolutional neural network for ta- ble and chart detection in digitized documents. In ICIAP, 2019.',\n",
              " '[23] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answer- ing for multimodal machine comprehension. In CVPR, 2017.',\n",
              " '[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.',\n",
              " '[25] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classiﬁcation. Dataset available from https://storage.googleapis.com/openimages/web/index.html, 2017.',\n",
              " '[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Com- put. Vision, 2017.',\n",
              " '[27] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Ep- stein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken- ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answer- ing research. Transactions of the Association of Computa- tional Linguistics, 2019.',\n",
              " '[28] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual question answer- ing by reading text in images. In ICDAR, 2019.',\n",
              " '[29] Tri Nguyen et al. Ms marco: A human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016.',\n",
              " '[30] Rasmus Berg Palm, Ole Winther, and Florian Laws. Cloudscan-a conﬁguration-free invoice analysis system us- ing recurrent neural networks. In ICDAR, 2017.',\n",
              " '[31] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.',\n",
              " '[32] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine com- prehension of text. arXiv preprint arXiv:1606.05250, 2016.',\n",
              " '[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS. 2015.',\n",
              " '[34] Amanpreet Singh, Vedanuj Goswami, Vivek Natarajan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Mmf: A multimodal framework for vision and language research. https://github.com/ facebookresearch/mmf, 2020.',\n",
              " '[35] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019.',\n",
              " '[36] Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, and Anirban Chakraborty. From strings to things: Knowledge- enabled vqa model that can read and reason. In ICCV, 2019.',\n",
              " '[37] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Sule- man. Newsqa: A machine comprehension dataset. CoRR, abs/1611.09830, 2016.',\n",
              " '[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS. 2017.',\n",
              " '[39] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of ev- idence, and bilingual scene-text visual question answering. In CVPR, 2020.',\n",
              " '[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau- mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural lan- guage processing. ArXiv, abs/1910.03771, 2019.',\n",
              " '[41] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: General- ized autoregressive pretraining for language understanding. In NeurIPS. 2019.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
        "            img.append(str(element))"
      ],
      "metadata": {
        "id": "mWkaE4HlOixW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UZeDqfzQO4dG",
        "outputId": "95aeef6d-a2d2-4706-f93f-46de4c88cac9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Geek Westen Sa gay Ce Rae Gal \\\\ noer ) Lolo - Cor]8D'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NW0IZkXLO5YY",
        "outputId": "21eddfa3-f734-46a1-8f9d-aeebfc9a697a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“Tobacco -Food : Drug -Fossil Fuel «Chemical ‘seo Number of Documents — 69 90\" Pees ot Number of Documents SES, GIES P PEL ES alll 6 50 s we ee “s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements2=partition_pdf(\n",
        "    filename=\"/content/data2/RAG_with_NLP_Tasks.pdf\",\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data2\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFkRgzsdPDrb",
        "outputId": "5bb927f1-a2b5-40be-c7da-b525a5d1b4c8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Header2=[]\n",
        "Footer2=[]\n",
        "Title2=[]\n",
        "NarrativeText2=[]\n",
        "Text2=[]\n",
        "ListItem2=[]\n",
        "img2 = []\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements2:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem2.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Image\" in str(type(element)):\n",
        "            img2.append(str(element))"
      ],
      "metadata": {
        "id": "iVUxH1p4PfV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Y0eiqlyWUxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}